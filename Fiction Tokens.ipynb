{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import htrc_features\n",
    "import htrc_features.resolvers\n",
    "from htrc_features import Volume, resolvers, FeatureReader, caching\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# A r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A resolver chain. Works from the bottom up; so first looks for a feather file in /drobo/hathi-ef; if it doesn't exist, creates it from\n",
    "the json.bz2 file in the same directory; if **that** doesn't exist, fetches over http. Will likely need to be edited.\n",
    "In general, my preference is for this stuff to live in a local json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_resolver = resolvers.make_resolver_chain([\n",
    "    {\"method\": \"http\"},\n",
    "    {\"method\": \"stubbytree\", \"format\": \"json\", \"compression\": \"bz2\", \"dir\": \"/drobo/hathi-ef\"},\n",
    "    {\"method\": \"stubbytree\", \"format\": \"feather\", \"compression\": \"zstd\", \"dir\": \"/drobo/hathi-ef\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# All the hathi book swith 'novel' in the title.\n",
    "novels = pd.read_csv(\"novels.csv\", names = [\"title\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = novels.sample(frac = 1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dorothy Forster : a novel /--- v.1</td>\n",
       "      <td>uiuo.ark:/13960/t5h99d66w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lost for love : a novel /--- v.3</td>\n",
       "      <td>uiuo.ark:/13960/t19k4w11x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Hampstead mystery : a novel /--- v.3</td>\n",
       "      <td>uiuo.ark:/13960/t2988r476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The king's mirror; a novel,</td>\n",
       "      <td>uc2.ark:/13960/t07w6fx15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The coast of Bohemia; a novel,</td>\n",
       "      <td>coo1.ark:/13960/t8x92rx3w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title                         id\n",
       "0        Dorothy Forster : a novel /--- v.1  uiuo.ark:/13960/t5h99d66w\n",
       "1          Lost for love : a novel /--- v.3  uiuo.ark:/13960/t19k4w11x\n",
       "2  The Hampstead mystery : a novel /--- v.3  uiuo.ark:/13960/t2988r476\n",
       "3               The king's mirror; a novel,   uc2.ark:/13960/t07w6fx15\n",
       "4            The coast of Bohemia; a novel,  coo1.ark:/13960/t8x92rx3w"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "   0________________________________________________________________________________|1\n",
      "   1________________________________________________________________________________|2\n",
      "   2________________________________________________________________________________|3\n",
      "   3________________________________________________________________________________|4\n",
      "   4________________________________________________________________________________|5\n",
      "   5________________________________________________________________________________|6\n",
      "   6________________________________________________________________________________|7\n",
      "   7________________________________________________________________________________|8\n",
      "   8________________________________________________________________________________|9\n",
      "   9________________________________________________________________________________|10\n",
      "  10________________________________________________________________________________|11\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "output = gzip.open(Path(\"input.unigrams.txt.gz\").expanduser(), \"w\")\n",
    "catalog = open(Path(\"jsoncatalog.txt\").expanduser(), \"w\")\n",
    "\n",
    "for i, id in enumerate(n.id):\n",
    "    print(i)\n",
    "    if i > 10:\n",
    "        # This is just demo code.\n",
    "        break\n",
    "    # A progress bar.\n",
    "    fract = int(80*i/n.shape[0])\n",
    "    print(f\"\\r{i:4}\" + \"=\" * fract + \"_\" * (80 - fract) + \"|\", end=\"\")\n",
    "    # end progress bar.\n",
    "    \n",
    "    # Open the volume with htrc-feature-reader\n",
    "    try:\n",
    "        v = Volume(id, id_resolver=my_resolver)\n",
    "    except FileNotFoundError:\n",
    "        print(id, \"not found\")\n",
    "        continue\n",
    "    # Start the metadata field we'll fill up.\n",
    "    meta = {}\n",
    "    for k in ['id', 'pub_date', 'language', 'access_profile', 'genre', 'contributor', 'handle_url',  'lcc', 'source_institution', 'pub_place', 'title', 'publisher']:\n",
    "        meta[k] = v.parser.meta[k]\n",
    "    try:\n",
    "        meta['first_author_birth'] = int(re.findall(\"[0-9]{4}\", meta['contributor'])[0])\n",
    "    except:\n",
    "        pass\n",
    "        # print(meta)\n",
    "    try:\n",
    "        # For this project, I only wanted English stuff.\n",
    "        \n",
    "        if not \"language\" in meta:\n",
    "            continue\n",
    "        if not \"eng\" in meta['language']:\n",
    "            continue\n",
    "            \n",
    "        # IMPORTANT--getting the tokenlist out. Here I'm building a bookworm\n",
    "        # where the documents are 5000 word chunks.\n",
    "            \n",
    "        chunked = v.tokenlist(chunk = True, chunk_target = 5000)\\\n",
    "          .reset_index().groupby([\"chunk\", \"token\"])['count']\\\n",
    "          .sum().reset_index().groupby(\"chunk\")\n",
    "        for ix, group in chunked:\n",
    "            # For each chunk. \n",
    "            \n",
    "            # IMPORTANT Using pandas to_csv with a formfeed line terminator to give the output type that Bookworm wants.\n",
    "            \n",
    "            tokencount = group[[\"token\", \"count\"]].to_csv(header = False, line_terminator=\"\\f\", index = False)\n",
    "            \n",
    "            # Tab is a valid character in some tokens in htrc-features, but screws things up for Bookworm. Kludgy.\n",
    "            tokencount = tokencount.replace(\"\\t\",\"{tab}\")\n",
    "            \n",
    "            # Populate the metadata more.\n",
    "            meta['filename'] = meta['id'] + \"-\" + str(ix)\n",
    "            meta['searchstring'] = f\"<a href={meta['handle_url']}>{meta['title']}</a>\"\n",
    "            meta['third'] = 1 + int(3 * ix/(len(chunked) + 1e-4))\n",
    "            meta['sixth'] = 1 + int(6 * ix/(len(chunked) + 1e-4))\n",
    "            meta['twelfth'] = 1 + int(12 * ix/(len(chunked) + 1e-4))\n",
    "            \n",
    "            # Write out the csv to input.unigrams.txt.gz\n",
    "            output.write(f\"{meta['filename']}\\t{tokencount}\\n\".encode(\"utf-8\"))\n",
    "            catalog.write(json.dumps(meta) + \"\\n\")\n",
    "    except:\n",
    "        print(\"Some error on\", id)\n",
    "        continue\n",
    "output.close()\n",
    "catalog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Volume(id, id_resolver=my_resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.tokenlist().sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.close()\n",
    "catalog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{\n",
    "  \"plottype\": \"linechart\",\n",
    "  \"smoothingSpan\": 0,\n",
    "  \"host\": \"http://localhost:10012/\",\n",
    "  \"database\": \"fiction\",\n",
    "  \"aesthetic\": {\n",
    "    \"x\": \"sixth\",\n",
    "    \"y\": \"WordsPerMillion\"\n",
    "  },\n",
    "  \"search_limits\": {\"word\":[\"sleep\"]},\n",
    "  \"vega\": {\n",
    "    \"width\": 400,\n",
    "    \"title\": \"Number of books in the Hathi trust by year, top 12 languages.\"\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
